---
title: "PDA_EDA"
author: "Maria Pia El Asmar & Hasini Gunawardena"
date: "10/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Packages to be put in the next section at the end of the project
```{r setup, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(DT)
library(Hmisc)  
library(tidyquant)
library(ggthemes)
library(RColorBrewer)
library(corrplot)
library(psych)
library(GGally)
library(corrr)
library(corrplot)
library(ggcorrplot)
library(kableExtra)
#library(DataExplorer)
library(inspectdf)
library(explore)
library(dplyr)
```


```{r setup2, echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  fig.retina = 0.8, # figures are either vectors or 300 dpi diagrams
  dpi = 300,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  
  fig.show = "hold"
)

ggplot2::theme_set(ggplot2::theme_light())

options(
  digits = 3,
  str = strOptions(strict.width = "cut"),
  width = 69,
  tibble.width = 69,
  cli.unicode = FALSE
)
```


# Introduction

```{r, echo=FALSE}
GermanCredit <- read.csv(
  here::here("data/GermanCredit.csv"),
  header = TRUE,
  sep = ";",
  dec = "."
)[,-1]

```


# Exploratory Data Analysis

## Structure & Summary

We are going to start our exploratory data analysis by understanding the data. First, let us observe the type of variables of the dataset:

```{r, echo=FALSE}
str(GermanCredit)
```

We note that all the variables are integer. However, from the data description, a lot of those are actually categorical variables. Hence, in order to have consistent results in our analysis, we are going to transform these variables from integer to factors.

To transform integer variables into correct categorical ones, we use a for loop. 
While analysing the data, we noted that the variable "EDUCATION" has one outlier: indeed, in one observation it is equal to "-1" instead of 0/1. For this reason, we change the value of this observation. 

Moreover, we noted that there is an outlier also for the variable "AGE" and the variable "GUARANTOR". For the first mentionned, we changed the observation having a value of 125 to 75, while for the guarantor we changed one observation from the value of 2 to 1.

```{r, echo=FALSE}

# changing education
for (i in 1:1000){
  if (GermanCredit[i,8] == "-1") {
    GermanCredit[i,8] <- 1
    GermanCredit[,8] <- as.factor(GermanCredit[,8])
  }
}

# changing guarantor
for (i in 1:1000){
  if (GermanCredit[i,18] == "2") {
    GermanCredit[i,18] <- 1
    GermanCredit[,18] <- as.factor(GermanCredit[,18])
  }
}

# changing age
for (i in 1:1000){
  if (GermanCredit[i,22] == "125") {
    GermanCredit[i,22] <- 75
  }
}

for (i in (1:31)) {
  if (is.integer(GermanCredit[,i]) & i != 2 & i !=10  & i !=22) {
  GermanCredit[,i] <- as.factor(GermanCredit[,i])
  }
}

for (i in c(2,10,22)){
  GermanCredit[,i] <- as.numeric(GermanCredit[,i])
}
```


```{r, echo=FALSE}
library(DT)
str(GermanCredit)

datatable(head(GermanCredit), class = "cell-border stripe") #Ca apparait pas haha 
# PIA: Ã  checker quand on va knit
```

Now that the variables are in the correct form, we can move on with our analysis.

We start by looking at the structure of the dataset.

```{r, echo=FALSE}
library(DataExplorer)
plot_intro(GermanCredit)

types<-inspect_types(GermanCredit)
show_plot(types, col_palette=2)
```

From the first plot, we note that there are no missing observations. Moreover, after the variables transformation 87% of them have discrete values, meaning that these are categorical variables. That is confirmed by the second plot that gives us the exact number of columns which are integer and the ones that are factors (categorical variables).

We are now going to dive deeper into the variables' analysis. 

```{r, echo=FALSE}
library(psych)
library(Hmisc)
summary(GermanCredit)

```

The summary gives us a first overview of the variables' details. Interestingly, some categorical variables present an unequal distribution of values. For instance, we note that the variable "CO.APPLICANT" has 959 times "0" and only 41 times "1". A similar comment can be made for other categorical variables like "MALE_MAR_or_WID", "FOREIGN", "MALE_DIV", "EDUCATION". 


Let us now analyse the numerical variables. For those, we can look at the min and the max, which gives us a range. For instance, we see that in this bank the duration of the loan goes from 4 to 72 months. The larger the range, the higher the chance of having outliers. For example, the "AMOUNT" variable has a very big range: from 250 to 18'424. Moreover, we note that for all the numerical variables the median is lower than the mean. For instance, "AMOUNT" has a median of 2320 and a mean of 3271, which will mean that the histogram will present a long right tail. 

```{r, echo=FALSE}
library(inspectdf)

cat.var<-inspect_cat(GermanCredit) 
show_plot(cat.var) 

```

This plot allows us to have a better visualization of how the factor variables are distributed. We confirm that for most of them, the number of instances in each level of the variables are unbalanced. The only variable that seems to have a similar distribution of 1 and 0 is MALE_SINGLE.


## Histogram & Density

```{r, echo=FALSE}
library(ggplot2)
#new
num.var<-inspect_num(GermanCredit)
show_plot(num.var, col_palette=3)

plot_density(GermanCredit[,-31],
             ggtheme = theme_bw())


```

We are going to comment the histogram for each variable. 
First, let us look at AGE. Most of the people taking the credit are between their 20s and 40s. However, the age range is quite large and it is not uncommon to see people over 60 getting a credit in this dataset. 

Second, we shall analysis AMOUNT. This variable is mostly concentrated between 0 and 5000. It looks like in this dataset in it rare to have a high credits (> 10'000).

Third, we look at the DURATION of the credits. Most of them are going to last between 5 and 20 years. Sometime the duration can last even longer, and we can image that this will be the case for high credits. 

Lastly, by observing the histograms we note that the three variables are skewed on the right. This makes us think that there is a big difference between the mean and the median. Looking back to the summary, we see that indeed the mean and the median of all the variables are quite different.

## Side-by-side boxplots

```{r, echo=FALSE}

lblue <- "#6699CC"
par(mfrow = c(2, 3))

boxplot(AGE ~ RESPONSE, data = GermanCredit, xlab = "Response", notch = T, 
    varwidth = T, col = lblue)
boxplot(AMOUNT ~ RESPONSE, data = GermanCredit, xlab = "Response", varwidth = T, col = lblue)
boxplot(DURATION ~ RESPONSE, data = GermanCredit, xlab = "Response", varwidth = T, col = lblue)
#boxplot(INSTALL_RATE ~ RESPONSE, data = GermanCredit, xlab = "Response", varwidth = T, col = lblue)
#boxplot(NUM_CREDITS ~ RESPONSE, data = GermanCredit, xlab = "Response", varwidth = T, col = lblue)


```
The boxplot confirm what stated before. For each variable, we see in which range most values stand according the outcome variable RESPONSE. Moreover, it allows us to have a visual representation of possible outliers: for instance, we see that for AMOUNT there is one observation where the credit is really high, and the outcome variable RESPONSE is 0. 

The boxplots allow also to compare the median of the variables according the RESPONSE. On the one hand, AMOUNT has almost the same midian regardless of the outcome. On the other hand both AGE and DURATION show a slightly different median according to RESPONSE. Moreover, we can look at the range of each variable and highlight that the latter is also quite different according to the outcome, as we can clearly see for DURATION. To further develop, the range for RESPONSE = 1 is smaller for both AMOUNT and DURATION and more or less equivalent to RESPONSE = 0 for AGE.

## Good vs Bad credits
```{r, echo=FALSE}

#CHECK QUE TOUTES LES VARIABLES ON ETE CONSIDERE
GermanCredit %>% 
  select(TELEPHONE, RADIO.TV, RESPONSE) %>% 
  explore_all(target = RESPONSE)

GermanCredit %>% 
  select(NEW_CAR, USED_CAR,FURNITURE,EDUCATION, RESPONSE) %>% 
  explore_all(target = RESPONSE)

GermanCredit %>% 
  select(FOREIGN, MALE_DIV, MALE_SINGLE, MALE_MAR_or_WID, RESPONSE) %>% 
  explore_all(target = RESPONSE)

GermanCredit %>% 
  select(GUARANTOR, CO.APPLICANT, PROP_UNKN_NONE, OTHER_INSTALL, RESPONSE) %>% 
  explore_all(target = RESPONSE)

GermanCredit %>% 
  select(REAL_ESTATE, RENT, OWN_RES, RETRAINING, RESPONSE) %>% 
  explore_all(target = RESPONSE)

GermanCredit %>% 
  select(NUM_CREDITS, NUM_DEPENDENTS, INSTALL_RATE, RESPONSE) %>% 
  explore_all(target = RESPONSE)

```
Similarly to what we saw in the boxplot for the integer variables, we now observe the distribution of all the catgorical ones according to the outcome variable RESPONSE.

We note that there are no discrimination for any categorical variable when considering the RESPONSE feature, meaning that we are not going to be able to define RESPONSE only by looking at those variables indivdually.

After having deeply analysed the dataset, we are now going to look at the correlation between the variables and then proceed with the model analysis.

## Correlations

```{r, echo=FALSE, fig.width= 8, fig.height= 7}
library(DataExplorer)
plot_correlation(GermanCredit, type= 'c', cor_args = list( 'use' = 'complete.obs')) #Library DataExplorer
```


From the correlation matrix we see that only "DURATION" and "AMOUNT" are correlated. Moreover, they are positively correlated. When working on the models we should be aware of this correlation and eventually compute the VIF coefficient in order to be sure that there is no multicollinearity problems.

It seems like there is no correlation between the other variables, meaning that they are independent, which is actually positive since it means that for these variables we will not have problems of multicollinearity. 






