---
title: "Modeling"
author: "Maria Pia El Asmar & Hasini Gunawardena"
date: "10/30/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Boruta Algorithm

```{r}
library(Boruta)
set.seed(1)
boruta.train <- Boruta(RESPONSE ~., data =GermanCredit, doTrace = 2)
final.boruta <- TentativeRoughFix(boruta.train)
print(final.boruta)


# comment on voit les variables manquantes?
```

# Partition of Data

To begin the modeling section, we start by partitioning the data set into a training set (750 observations) and testing set (250 observations), selected at random.

```{r, echo=FALSE}

row.order <- sample(c(1:1000)) # first randomize the order of the rows
german.tr <- GermanCredit[row.order[1:750],] # take the first 750 (random) rows of german for the training set
german.te <- GermanCredit[row.order[751:1000],]


```

In order to be consistent, we decided to build all models using the caret package.

# Decision Tree with Caret

The first model that we investigate is the decision tree. The model is built on the training set and then the test set is used to measure the prediction capacity of the model. The data is normalised through the preProcess argument. We use the repeated cross-validation method, in order to achieve better results. Also, we balance the data thanks to "sampling = down", to ensure that the prediction capacity on both classes is balanced. Lastly, we perform a tuning of the complexity parameter (cp), to further improve the model.

```{r, echo=FALSE}
library(caret)
set.seed(12)
hp_ct <- data.frame(cp = seq(from = 0.03, to = 0, by = -0.003))
ct.caret <- train(
  RESPONSE ~ .,
  data = german.tr,
  method = "rpart",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    verboseIter = FALSE,
    sampling = "down"
  ),
  tuneGrid = hp_ct
)

```

## Best Complexity parameter 
```{r, echo=FALSE}
ct.caret$bestTune 
print(ct.caret)
plot(ct.caret)

```

By the information provided above, we can see that the best accuracy on the training set is of 65.9%. This is achieved by using a complexity parameter of 0.012.

  ## Confusion Matrix

We are now going to look on the performance on the test set.

```{r, echo=FALSE}
confusionMatrix(predict.train(ct.caret, newdata = german.te), 
                german.te$RESPONSE)


``` 

As we can see, the overall accuracy on the test set is of 66.4%. However, one can observe that even though the prediction capacity on each class is more balanced than if we had not balanced the data, the specificity class is better predicted (67.8%) compared to the sensitivity class (62.7%). This is confirmed in the confusion matrix as well. To further explain, the "1" class is correctly predicted 124 times out of 183 times. Whereas, the "0" class is correctly predicted 42 times out of 67 times. This means that the model performs well in predicting good credit rating candidates. However, if the aim of the project is to uncover bad credit rating candidates based on the explanatory variables, the model will do a poorer job.

  ## Tree drawing
  
We now plot the final and best model. 
```{r, echo=FALSE}
plot(ct.caret$finalModel)
text(ct.caret$finalModel)

```

# Neural Network with Caret

The second model that we considered is the Neural Network. Similarly to the decision tree, we scaled and balanced the data, as well as, preformed a repeated cross-validation, in order to achieve better results. To go further, we also tuned the "size" and "decay" hyperparameters.

```{r, echo=FALSE, include = FALSE}
set.seed(2006)

hp_nn <- expand.grid(size = 2:4, decay = seq(0, 0.5, 0.05))

credit.nn.caret <- train (
  form = RESPONSE ~ .,
  data = german.tr,
  method = "nnet",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    verboseIter = FALSE,
    sampling = "down"),
  tuneGrid = hp_nn
)

#final  value 215.167737 
#stopped after 100 iterations

```


## Accuracy measure
  
```{r, echo=FALSE}
credit.nn.caret$bestTune 
print(credit.nn.caret)
plot(credit.nn.caret)
```
As we can see, the highest accuracy (68.9%) on the training set can be obtained by setting size = 2 and decay = 0.5.

## Confusion Matrix
```{r, echo=FALSE}
confusionMatrix(predict.train(credit.nn.caret, newdata = german.te), 
                german.te$RESPONSE)
``` 
The results on the training set are quite good. Overall we obtain an accuracy of 72.4%. The prediction of each class is balanced even though the specificity class (75.4%) is better predicted than the sensitivity class (64.2%), similarly to the Decision Tree model. 

The results are confirmed in the confusion matrix where the "1" class is correctly predicted 138 times out of 183 times. Whereas, the "0" class is correctly predicted 43 times out of 67 times. In this way, the Neural Network outperforms the Decision Tree model.

  ## Neural Network drawing
```{r, echo=FALSE}
library(NeuralNetTools)
plotnet(credit.nn.caret, pos_col = "darkgreen", neg_col = "darkblue")

```

# Logistic regression with caret

The third model that we considered is the Logistic Regression, that is a powerful classifier for binary outputs.

```{r, echo=FALSE, warning = FALSE}
set.seed(1)
credit.log.caret <-
  train(
    form = RESPONSE ~ .,
    data = german.tr,
    method = "glm",
    preProcess = c("center", "scale"),
    trControl = trainControl(
      method = "repeatedcv",
      number = 10,
      repeats = 10,
      verboseIter = FALSE,
      sampling = "down"
    )
  )


```


## Accuracy measure
  
```{r, echo=FALSE}
print(credit.log.caret)
summary(credit.log.caret)


```
The results of the training show that not all variables significantly impact the RESPONSE variable. For instance, according to the model, variables such as JOB or TELEPHONE do not impact the RESPONSE variable significantly. Other variables may impact the explained variable at different thresholds. For example, CHK_ACCT3 has a significant influence at a threshold of 0.1%, whereas HISTORY OTHER_INSTALL has an impact a threshold of 5% and REAL_ESTATE at 10%.

Overall, the model presents a good accuracy (69.7%) on the training set. However, to truly evaluate its prediction capacity we would need to consider the accuracy on the test set.

## Confusion Matrix
```{r, echo=FALSE}
confusionMatrix(predict.train(credit.log.caret, newdata = german.te),
                german.te$RESPONSE)


``` 
The analysis of the predictions on the test set reveals that it is as well a good model since the overall accuracy is of 70.4%. This means, that overall the Logistic Regression outperforms the Decision Tree but underperforms compared to the Neural Network.

When looking in more details, we can see that both classes are more or less well predicted. In contrary to the two previous models, this time the sensitivity class (74.6%) is better predicted than the specificity class (68.9%). The results on the confusion matrix confirm this. To further explain, the "1" class is correctly predicted 126 times out of 183. Whereas, the "0" class is accurately predicted 50 times out of 67.

This means, that for predicting the specificity class ("1"), the Logistic Regression performs better than the Decision Tree, but underperforms compared to the Neural Network. When looking at the sensitivity class ("0"), this model outperforms both the Neural Network and the Decision Tree. In other words, when only considering these three models seen up to now, the Neural Network would be the better model to find good credit candidates. Whereas, if the aim of the project is to detect bad credit candidates, then the Logistic Regression model should be preferred.

Let's look further into other models.

# Discriminant Analysis

```{r, echo=FALSE, message= FALSE}
#LDA
# library(MASS)
# lda.fit <- lda(RESPONSE ~ ., data = GermanCredit)
# lda.fit


lda.fit <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="lda",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling = "down")
                         )

lda.fit
```


## Predictions
```{r, echo=FALSE}
lda.pred <- predict(lda.fit, GermanCredit)
names(lda.pred) 

#head(lda.pred$posterior)

```

## Confusion Matrix
```{r, echo=FALSE}
confusionMatrix(predict.train(lda.fit, newdata = german.te),
                german.te$RESPONSE)
```

```{r, echo=FALSE}
# remove
# library(tidyverse)
# library(ggthemes)
# prev.prob.lda$class <- lda.class
# prev.prob.lda$correct <- ifelse(prev.class.lda$actual == prev.class.lda$class, TRUE, 
#     FALSE)
# ggplot(prev.prob.lda, aes(x = class, fill = correct)) + 
#   geom_bar(position = "dodge") + 
#   scale_fill_brewer(palette = "Set1") + 
#   theme_get()
```

# QDA

```{r, echo=FALSE, warning= FALSE}
# qda.fit <- qda(RESPONSE ~ ., data = GermanCredit)
# qda.fit
# 
# #ca me donne une erreure je comprends pas d'oÃ¹ ca viennnnnt
# qda.class <- predict(qda.fit, GermanCredit$RESPONSE)$class 
# table(qda.class, Direction.2005)
set-seed(1)
qda.fit <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="qda",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling = "down")
                         ) 

qda.pred <- predict(qda.fit, GermanCredit)

confusionMatrix(predict.train(qda.fit, newdata = german.te),
                german.te$RESPONSE)

```
    
# KNN

```{r, echo=FALSE}


```

# Support Vector Machine 

```{r, echo=FALSE, warning = FALSE}
library(caret)
model_svm <- caret::train(RESPONSE ~ .,
                          data = german.tr, 
                          method = "svmRadialCost",
                          preProcess = "range",
                          trace = FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE))
model_svm

# selecting the best tune
model_svm$bestTune
plot(model_svm)


C <- c(0.25, 0.1, 0.5, 1, 10, 100)
sigma <- c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C = C, sigma = sigma)
model_svm.1<-caret::train(RESPONSE ~ .,
                          data = german.te,
                          method = "svmRadial",
                          preProcess = "range",
                          trace=FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE),
                          tuneGrid=gr.radial)
model_svm.1
model_svm.1$bestTune
plot(model_svm.1)
model_svm_pred <- predict(model_svm, german.te)
CrossTable(x = german.te$RESPONSE, y = model_svm_pred, prop.chisq = FALSE)

confusionMatrix(predict.train(model_svm, newdata = german.te),
                german.te$RESPONSE)

confusionMatrix(predict.train(model_svm.1, newdata = german.te),
                german.te$RESPONSE)
```

# Ensemble methods (Random Forest)

```{r, echo= FALSE, message = FALSE}
library(randomForest)
library(funModeling)
library(lessR)
library(gmodels)

form <- formula (RESPONSE ~.)

# Call the function:
integ_mod_1 <- data_integrity_model(data = GermanCredit, model_name = "randomForest")
integ_mod_1$data_ok

CreditRF <- randomForest(formula=form, 
                          data=german.tr, 
                          ntree=500, mtry=4, 
                          importance=TRUE, 
                          localImp=TRUE,
                          na.action=na.roughfix,
                          replace=FALSE)

print(CreditRF)

## Variable importance analysis

head(round(importance(CreditRF), 2))

details(varImpPlot(CreditRF))

## Diagnostic model: error rate
round(head(CreditRF$err.rate, 15), 4)

details(plot(CreditRF))

## Confusion matrix

CreditRF$confusion

## Confusion matrix for the test set

CreditRF.pred<-predict(CreditRF, german.te, type="class")
CreditRF.pred

table(true=GermanCredit$RESPONSE[-c(1:750)], pred=CreditRF.pred) #PIA: est ce que c est juste de faire (c(1:750))??

CrossTable(x=GermanCredit$RESPONSE[-c(1:750)], y=CreditRF.pred, prop.chisq=FALSE)

```

## RF with repeated cross-validation

```{r, echo = FALSE, warning=FALSE}
library(caret)
modelLookup(model="rf")

#ca prend beaucouuuup de temps mais a la fin ca marche ahah
  model_rf <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="rf",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling="down")
                         )


model_rf

model_rf$bestTune

# mtry : 

# confusion matrix
confusionMatrix(predict.train(model_rf, newdata = german.te),
                german.te$RESPONSE)

```

## Variable importance analysis

```{r, echo = FALSE}

# ici on devra mettre le alg. BORUTA
varImp(model_rf)
head(round(importance(model_rf), 2))



``` 
## Confusion matrix

```{r, echo = FALSE}
library(gmodels)
model_rf_pred <- predict(model_rf, german.te)
confusionMatrix(model_rf_pred, german.te$RESPONSE)

CrossTable(x=german.te$RESPONSE, y=model_rf_pred, prop.chisq=FALSE)

```





