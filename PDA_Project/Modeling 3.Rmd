---
title: "Modeling"
author: "Maria Pia El Asmar & Hasini Gunawardena"
date: "10/30/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Boruta Algorithm

Variable selection is an important part of the model building process. To decide on which variables to use, we employ the Boruta Algorithm. In contrary to other traditional feature selection models, that only rely on a small subset of features for the selection, this algorithm captures all features relevant to the outcome variable. (https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/)

```{r, message=FALSE, warning=FALSE}
library(Boruta)
set.seed(1)
boruta.train <- Boruta(RESPONSE ~., data =GermanCredit, doTrace = 2)
print(boruta.train)

```
According to the Boruta algorithm, there are 15 attributes that are confirmed as important, 11 as unimportant and 4 that are left as tentative. The latter means that the Boruta model is unable to decide whether those variables are important or not. 

## Plotting the most important variables

By plotting the most important variables according to the Boruta algorithm, one can observe the individual importance of each feature. For instance, CHK_ACCT appears as the most important one, followed by DURATION and HISTORY. The green variables are the ones considered as important, while the red ones are the unimportant features. Additionally, the yellow ones are the four tentative variables mentioned previously. Those are: JOB, NUM_CREDIT, RENT and FOREIGN.

```{r}
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)
title(main="Variable Importance According to the Boruta Algorithm")


```


## Tentative Rough Fix

We use the TentativeRoughFix() function in order to make a decision on the tentative variables and classify them as either important or unimportant.

```{r}
set.seed(2)
final.boruta <- TentativeRoughFix(boruta.train)
print(final.boruta)

```

## Plotting the variable importance after the fix

```{r}
plot(final.boruta, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(final.boruta$ImpHistory),function(i)
final.boruta$ImpHistory[is.finite(final.boruta$ImpHistory[,i]),i])
names(lz) <- colnames(final.boruta$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(final.boruta$ImpHistory), cex.axis = 0.7)
title(main="Variable Importance After the Fix")


```
As we can see, JOB and NUM_CREDITS are now classified as important features. Whereas, RENT and FOREIGN have been classified as unimportant.

## Focus on Important Variables

Now that we have selected which features to use, that are the important variables, we are now going to filter the dataset in order to keep only those attributes.

```{r}
GermanCredit <-
  GermanCredit %>% select(
    "RESPONSE",
    "CHK_ACCT",
    "DURATION",
    "HISTORY",
    "AMOUNT",
    "SAV_ACCT",
    "GUARANTOR",
    "OTHER_INSTALL",
    "EMPLOYMENT",
    "AGE",
    "USED_CAR",
    "REAL_ESTATE",
    "PROP_UNKN_NONE",
    "NEW_CAR",
    "OWN_RES",
    "INSTALL_RATE",
    "JOB",
    "NUM_CREDITS"
  )


```


# Partition of Data

To begin the modeling section, we start by partitioning the data set into a training set (750 observations) and testing set (250 observations), selected at random.

```{r, echo=FALSE}

row.order <- sample(c(1:1000)) # first randomize the order of the rows
german.tr <- GermanCredit[row.order[1:750],] # take the first 750 (random) rows of german for the training set
german.te <- GermanCredit[row.order[751:1000],]


```

In order to be consistent, we decided to build all models using the caret package.

# Decision Tree

The first model that we investigate is the decision tree. The model is built on the training set and then the test set is used to measure the prediction capacity of the model. The data is normalised through the preProcess argument. We use the repeated cross-validation method, in order to achieve better results. Also, we balance the data thanks to "sampling = down", to ensure that the prediction capacity on both classes is balanced. Lastly, we perform a tuning of the complexity parameter (cp), to further improve the model.

```{r, echo=FALSE, message = FALSE}
library(caret)
set.seed(12)
hp_ct <- data.frame(cp = seq(from = 0.03, to = 0, by = -0.003))
ct.caret <- train(
  RESPONSE ~ .,
  data = german.tr,
  method = "rpart",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    verboseIter = FALSE,
    sampling = "down"
  ),
  tuneGrid = hp_ct
)

```

## Best Complexity parameter 
```{r, echo=FALSE}
ct.caret$bestTune 
print(ct.caret)
plot(ct.caret)

```

By the information provided above, we can see that the best accuracy on the training set is of 65.6%. This is achieved by using a complexity parameter of 0.015.

## Confusion Matrix

We are now going to look on the performance on the test set.

```{r, echo=FALSE}
confusionMatrix(predict.train(ct.caret, newdata = german.te), 
                german.te$RESPONSE)


``` 

As we can see, the overall accuracy on the test set is of 60.4%. However, one can observe that even though the prediction capacity on each class is more balanced than if we had not balanced the data, the sensitivity class is better predicted (65.9%) compared to the specificity class (57.7%). This is confirmed in the confusion matrix as well. To further explain, the "1" class is correctly predicted 97 times out of 168 times. Whereas, the "0" class is correctly predicted 54 times out of 82 times. This means that the model performs well in predicting bad credit rating candidates. However, if the aim of the project is to uncover good credit rating candidates based on the explanatory variables, the model will do a poorer job.

## Tree drawing
  
We now plot the final and best model. 
```{r, echo=FALSE}
plot(ct.caret$finalModel)
text(ct.caret$finalModel)
#PIA: on ne voit pas tres bien le graph
```

# Neural Network

The second model that we considered is the Neural Network. Similarly to the decision tree, we scaled and balanced the data, as well as, preformed a repeated cross-validation, in order to achieve better results. To go further, we also tuned the "size" and "decay" hyperparameters.

```{r, echo=FALSE, include = FALSE}
set.seed(2006)

hp_nn <- expand.grid(size = 2:4, decay = seq(0, 0.5, 0.05))

credit.nn.caret <- train (
  form = RESPONSE ~ .,
  data = german.tr,
  method = "nnet",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    verboseIter = FALSE,
    sampling = "down"),
  tuneGrid = hp_nn
)

#final  value 215.167737 
#stopped after 100 iterations

```


## Accuracy measure
  
```{r, echo=FALSE}
credit.nn.caret$bestTune 
print(credit.nn.caret)
plot(credit.nn.caret)
```
As we can see, the highest accuracy (69.3%) on the training set can be obtained by setting size = 4 and decay = 0.5.

## Confusion Matrix
```{r, echo=FALSE}
confusionMatrix(predict.train(credit.nn.caret, newdata = german.te), 
                german.te$RESPONSE)
``` 
The results on the test set are slightly better than for the Decision Tree. Overall we obtain an accuracy of 65.6%. The prediction of each class is balanced even though the specificity class (66.7%) is better predicted than the sensitivity class (63.4%), unlike for the Decision Tree model. 

The results are confirmed in the confusion matrix where the "1" class is correctly predicted 112 times out of 168 times. Whereas, the "0" class is correctly predicted 52 times out of 82 times. In this way, the Neural Network outperforms the Decision Tree model in predicting the specificity class (1). However, the results are almost equivalent for the sensitivity class (0).

## Neural Network Plotting
  
```{r, echo=FALSE}
library(NeuralNetTools)
plotnet(credit.nn.caret, pos_col = "darkgreen", neg_col = "darkblue")

```

# Logistic regression with caret

The third model that we considered is the Logistic Regression, that is a powerful classifier for binary outputs.

```{r, echo=FALSE, warning = FALSE}
set.seed(1)
credit.log.caret <-
  train(
    form = RESPONSE ~ .,
    data = german.tr,
    method = "glm",
    preProcess = c("center", "scale"),
    trControl = trainControl(
      method = "repeatedcv",
      number = 10,
      repeats = 10,
      verboseIter = FALSE,
      sampling = "down"
    )
  )


```


## Accuracy measure
  
```{r, echo=FALSE}
print(credit.log.caret)
summary(credit.log.caret)


```
The results of the training show that not all variables significantly impact the RESPONSE variable. For instance, according to the model, variables such as JOB or NEW_cAR do not impact the RESPONSE variable significantly. Other variables may impact the explained variable at different thresholds. For example, CHK_ACCT3 has a significant influence at a threshold of 0.1%, whereas EMPLOYMENT4, INSTALL_RATE4 and HISTORY4 have impacts at thresholds of, respectively, 1%, 5% and 10%.

Overall, the model presents a good accuracy (70.9%) on the training set. However, to truly evaluate its prediction capacity we would need to consider the accuracy on the test set.

## Confusion Matrix
```{r, echo=FALSE}
confusionMatrix(predict.train(credit.log.caret, newdata = german.te),
                german.te$RESPONSE)


``` 
The analysis of the predictions on the test set reveals that it is as well a good model since the overall accuracy is of 68.8%. This means, that overall the Logistic Regression outperforms both the Decision Tree and the Neural Network.

When looking in more details, we can see that both classes are more or less well predicted. Similarly to the Decision Tree model, the sensitivity class (73.2%) is better predicted than the specificity class (66.7%). The results on the confusion matrix confirm this. To further explain, the "1" class is correctly predicted 112 times out of 168. Whereas, the "0" class is accurately predicted 60 times out of 82.

This means, that for predicting the specificity class ("1"), the Logistic Regression performs better than the Decision Tree, but equals the performance of the Neural Network. When looking at the sensitivity class ("0"), this model outperforms both the Neural Network and the Decision Tree. In other words, when only considering these three models seen up to now, the Neural Network and the Logistic Regression would be the better models to find good credit candidates. On the other hand, if the aim of the project is to detect bad credit candidates, then the Logistic Regression model should be preferred. Overall, this confirms that the Logistic Regression does a better job than both the Neural Network and the Decision Tree.

Let's look further into other models.

# Discriminant Analysis

The fourth model considered is the Discriminant Analysis. First, we are going to use a linear discriminant analysis, then a quadratic discriminant analysis.

## LDA

```{r, echo=FALSE, message= FALSE}

lda.fit <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="lda",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling = "down")
                         )
```

After having trained the model, we are going to make predictions on the test set and look at the results.

### Confusion matrix
```{r, echo=FALSE}
lda.pred <- predict(lda.fit, GermanCredit)

confusionMatrix(predict.train(lda.fit, newdata = german.te),
                german.te$RESPONSE)
```

One can observe that the Accuracy is of 70.4%. So far, this is the best model. 

Looking into more details, we note that the Sensitivity and the Specificity are respectively of 71.95% and 69.64%. The model was able to predict 59 times out of 82 a bad credit, and 117 times out of 168 a good credit. Comparing with the other models, the LDA is the one that best predict the class ("1"), meaning the good credits. It also performs better at the prediction of class ("0") compared to the other models, except for the logitic regression.

## QDA
We are now going to create a model using a quadritic discriminant analysis.
```{r, echo=FALSE, warning= FALSE}

set.seed(1)
qda.fit <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="qda",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling = "down")
                         ) 


qda.fit
```
When training the model, the Accuracy is at 68% which is lower compared to the linear discriminant analysis model. We are still going to see how the model performs when fitted on the test dataset.

### Confusion Matrix
```{r}
qda.pred <- predict(qda.fit, GermanCredit)

confusionMatrix(predict.train(qda.fit, newdata = german.te),
                german.te$RESPONSE)

```
When the mdoel is fitted on the test set, the Accuracy is of 71.2%, which is the highest level achieved so far. However when looking at the Sensitivity, one can remark that the it has a low value (60.98%). The model is able has predicted 50 bad credits out of 82, making him the worst in terms of prediction capability of the class "0". On the other hand, the Specificity is of 76.19%, the highest value achieved. 
Hence, we can say that the model is very good at predicting the good credits ("1"), but very bad a predicting the bad ones. Even if the accuracy is high, it is preferable to use a model that is able to well predict both classes. 

# Support Vector Machine 

```{r, echo=FALSE, warning = FALSE}

model_svm <- caret::train(RESPONSE ~ .,
                          data = german.tr, 
                          method = "svmRadialCost",
                          preProcess = "range",
                          trace = FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE))
model_svm

# selecting the best tune
model_svm$bestTune
plot(model_svm)
```


```{r, echo=FALSE, warning = FALSE}

C <- c(0.25, 0.1, 0.5, 1, 10, 100)
sigma <- c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C = C, sigma = sigma)
model_svm.1<-caret::train(RESPONSE ~ .,
                          data = german.te,
                          method = "svmRadial",
                          preProcess = "range",
                          trace=FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE),
                          tuneGrid=gr.radial)
model_svm.1
model_svm.1$bestTune
plot(model_svm.1)
```

## Confusiona matrix
```{r, echo=FALSE, warning = FALSE}
confusionMatrix(predict.train(model_svm, newdata = german.te),
                german.te$RESPONSE)

confusionMatrix(predict.train(model_svm.1, newdata = german.te),
                german.te$RESPONSE)
```

# Ensemble methods (Random Forest)

The last model we are going to analyse is the Random Forest. 

```{r, echo = FALSE, warning=FALSE}

modelLookup(model="rf")

model_rf <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="rf",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling="down")
                         )
model_rf
```
The output shows that the best Accuracy can be reached when mtry = 34. Indeed, the model has been doing different simulation with different mtry (Number of variables randomly sampled as candidates at each split), and ended by fitting the model to the training set using mtry = 34. Indeed, when we run the best tune we obtain the following:


```{r}
model_rf$bestTune
```

We are now going to look at how the model behaves when predicting the output from the test set.

## Confusion matrix
```{r}
# confusion matrix
confusionMatrix(predict.train(model_rf, newdata = german.te),
                german.te$RESPONSE)
```
The model's Accuracy is of 70%.

When looking at the Sensitivity and the Specificity, one can note that the model is better at predicting the bad credits ("0"). Indeed, the Sensitivity is of 75.61% while the Specificity of 67.26%. So when we fitted as random forest, we remark that 113 times out of 168 the model is able to predict good credits and 62 times out of 82, a bad one. Overall, the model is satisfying as it is able to predicted both classes pretty well. 

Generally, to have even better results we could make a variable importance analysis. However, having used the Bortuta Algorithm at the beginning of our analysis, this step is no longer necessary. 

# Conclusion