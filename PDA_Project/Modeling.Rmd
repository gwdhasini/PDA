---
title: "Modeling"
author: "Maria Pia El Asmar & Hasini Gunawardena"
date: "10/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Partition of Data

```{r, echo=FALSE}
##Il faut qu'on balance nos data avant sinon ça va faire des erreurs plus tard
##Mais après le data set risque d'être petit?
    #library(plyr) --> count(GermanCredit, "RESPONSE")

# GermanCredit %>% 
#     group_by(RESPONSE) %>% 
#     count()

row.order <- sample(c(1:1000)) # first randomize the order of the rows
german.tr <- GermanCredit[row.order[1:750],] # take the first 750 (random) rows of german for the training set
german.te <- GermanCredit[row.order[751:1000],]

```

# Decision Tree
## Model

```{r, echo=FALSE}
library(rpart)
set.seed(123)

credit.ct <- rpart(RESPONSE ~ ., method = "class", data = german.tr, control = rpart.control(minsplit = 4, 
    cp = 1e-05), model = TRUE)

#summary(credit.ct) --> trop long 

par(pty = "s", mar = c(1, 1, 1, 1))
plot(credit.ct, cex = 1)
text(credit.ct, cex = 0.6)

#J'ai pas fait le recursive partitioning up close 

```

## Complexity Table

```{r, echo=FALSE}
printcp(credit.ct)
par(pty = "s")
plotcp(credit.ct)

#Lowest xerror = 0.83333
#1-SE Rule = 0.91781 + 0.055387 = 0.973197 corresponding to cp = 0.0060883  (38 splits) but for visibility purpose let's take the second best --> cp = 0.0159817 (8 splits)
# Il avait dit que size = splits + 1 ---> ce n'est pas le cas ici
```

## Tree Number 

```{r, echo = FALSE}
par(pty = "s")
with(credit.ct, plot(cptable[, 3], xlab = "Tree Number", ylab = "Resubstitution Error (R)", 
    type = "b"))

par(pty = "s")
with(credit.ct, plot(cptable[, 4], xlab = "Tree Number", ylab = "Cross-Validated Error (R(cv))", 
    type = "b"))

par(pty = "s")
plotcp(credit.ct)
with(credit.ct, {
    lines(cptable[, 2] + 1, cptable[, 3], type = "b", col = "red")
    legend(2.7, 1, c("Resub. Error", "CV Error", "min(CV Error)+1SE"), lty = c(1, 
        1, 2), col = c("red", "black", "black"), bty = "n", cex = 0.8)
})

##Il faut revoir ça, problème dans légende 

```

## Pruning 

```{r, echo=FALSE}
require(rpart.plot)
credit.prune <- prune(credit.ct, cp = 0.0159817)
summary(credit.prune)
rpart.plot(credit.prune, main = "", extra = 104, under = TRUE, faclen = 0) #On voit quand même rien du tout

```

## Predictions

```{r, echo=FALSE}
library(knitr)
library(caret)
credit.pred <- predict(credit.prune, german.te, type = "class")
kable(table(credit.pred, german.te$RESPONSE), caption = "Prediction table")
#on a de meilleurs résultat en prunant avec 38 splits plutôt que 8 mais on voit rien sur les graphs avec 38 splits.

confusionMatrix(credit.pred,german.te$RESPONSE)
#Accuracy: 0.73


``` 


# Neural Network


```{r, echo=FALSE}
library(nnet)
credit.nn<- nnet(RESPONSE ~ ., data = german.tr, size = 3, rang = 0.1, decay = 5e-04, 
    Hess = T, maxit = 200)
#Faut qu'on regarde pour tuner les hyperparamters

```

## Plot
```{r, echo = FALSE}
library(NeuralNetTools)
par(mar = numeric(4), family = "serif")
plotnet(credit.nn, pos_col = "darkgreen", neg_col = "darkblue")

```

## Predictions

```{r, echo=FALSE}
library(caret)
library(dplyr)
credit.pred.nn <- predict(credit.nn, german.te, type= "class")

kable(table(german.te$RESPONSE, credit.pred.nn), caption = "Prediction table")
confusionMatrix(credit.pred.nn, german.te$RESPONSE)

##problem ici: y a seulement une class dans le "cred.pred.nn" mais je sais pas pourquoi
#count(german.te, "RESPONSE")
#count(german.tr, "RESPONSE")

```


# Logistic regression 

```{r, echo=FALSE}
credit.log <- glm(RESPONSE ~ ., data = german.tr, family = binomial)
summary(credit.log)

```

## Finding a Better Model

```{r, echo=FALSE}
step(credit.log)

```
## Better model

```{r, echo=FALSE}
credit.log.better <- glm(RESPONSE ~ TELEPHONE + RADIO.TV + GUARANTOR + OTHER_INSTALL + MALE_SINGLE + RENT + FOREIGN + USED_CAR + AMOUNT + EDUCATION + DURATION + PRESENT_RESIDENT + NEW_CAR + HISTORY + SAV_ACCT + INSTALL_RATE + CHK_ACCT, data = german.tr, family = binomial)

summary(credit.log.better)

```

```{r, echo=FALSE}
coef(credit.log.better)
```

## Predictions
```{r, echo=FALSE}
credit.log.pred <- predict(credit.log.better, type = "response")
kable(table(credit.log.pred, german.te$RESPONSE), caption = "Prediction table")
confusionMatrix(credit.log.pred, german.te$RESPONSE)

##Même problème qu'avant --> je pense que c'est lié au imbalanced data

```


# Discriminant Analysis

```{r, echo=FALSE}


```

# KNN

```{r, echo=FALSE}


```

# Support Vector Machine 

```{r, echo=FALSE}


```

# Ensemble methods (Random Forest)

```{r, echo= FALSE}


```

