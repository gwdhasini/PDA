# Boruta Algorithm

Variable selection is an important part of the model building process. To decide on which variables to use, we employ the Boruta Algorithm. In contrary to other traditional feature selection models, that only rely on a small subset of features for the selection, this algorithm captures all features relevant to the outcome variable. (Source: https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/)

```{r, message=FALSE, warning=FALSE}
library(Boruta)
set.seed(1)
boruta.train <- Boruta(RESPONSE ~., data =GermanCredit, doTrace = 2)
print(boruta.train)

```
According to the Boruta algorithm, there are 15 attributes that are confirmed as important, 11 as unimportant and 4 that are left as tentative. The latter means that the Boruta model is unable to decide whether those variables are important or not. 

## Plotting the most important variables

By plotting the most important variables according to the Boruta algorithm, one can observe the individual importance of each feature. For instance, CHK_ACCT appears as the most important one, followed by DURATION and HISTORY. The green variables are the ones considered as important, while the red ones are the unimportant features. Additionally, the yellow ones are the four tentative variables mentioned previously. Those are: JOB, NUM_CREDIT, RENT and FOREIGN.

```{r}
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)
title(main="Variable Importance According to the Boruta Algorithm")

```


## Tentative Rough Fix

We use the TentativeRoughFix() function in order to make a decision on the tentative variables and classify them as either important or unimportant.

```{r}
set.seed(2)
final.boruta <- TentativeRoughFix(boruta.train)
print(final.boruta)

```

## Plotting the variable importance after the fix

```{r}
plot(final.boruta, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(final.boruta$ImpHistory),function(i)
final.boruta$ImpHistory[is.finite(final.boruta$ImpHistory[,i]),i])
names(lz) <- colnames(final.boruta$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(final.boruta$ImpHistory), cex.axis = 0.45)
title(main="Variable Importance After the Fix")


```
As we can see, JOB and NUM_CREDITS are now classified as important features. Whereas, RENT and FOREIGN have been classified as unimportant.

## Focus on Important Variables

Now that we have selected which features to use, that are the important variables, we are now going to filter the dataset in order to keep only those attributes.

```{r}
GermanCredit <-
  GermanCredit %>% select(
    "RESPONSE",
    "CHK_ACCT",
    "DURATION",
    "HISTORY",
    "AMOUNT",
    "SAV_ACCT",
    "GUARANTOR",
    "OTHER_INSTALL",
    "EMPLOYMENT",
    "AGE",
    "USED_CAR",
    "REAL_ESTATE",
    "PROP_UNKN_NONE",
    "NEW_CAR",
    "OWN_RES",
    "INSTALL_RATE",
    "JOB",
    "NUM_CREDITS"
  )


```


# Partition of Data

To begin the modeling section, we start by partitioning the data set into a training set (750 observations) and testing set (250 observations), selected at random.

```{r}

row.order <- sample(c(1:1000)) # first randomize the order of the rows
german.tr <- GermanCredit[row.order[1:750],] # take the first 750 (random) rows of german for the training set
german.te <- GermanCredit[row.order[751:1000],]


```

In order to be consistent, we decided to build all models using the caret package.

# Decision Tree

The first model that we investigate is the decision tree. The model is built on the training set and then the test set is used to measure the prediction capacity of the model. The data is normalised through the preProcess argument. We use the repeated cross-validation method, in order to achieve better results. Also, we balance the data thanks to "sampling = down", to ensure that the prediction capacity on both classes is balanced. Lastly, we perform a tuning of the complexity parameter (cp), to further improve the model.

```{r, message = FALSE}
set.seed(12)
hp_ct <- data.frame(cp = seq(from = 0.03, to = 0, by = -0.003))
ct.caret <- train(
  RESPONSE ~ .,
  data = german.tr,
  method = "rpart",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    verboseIter = FALSE,
    sampling = "down"
  ),
  tuneGrid = hp_ct
)

```

## Best Complexity Parameter 
```{r}
ct.caret$bestTune 
print(ct.caret)
plot(ct.caret, main = "Best Complexity Parameter")

```

By the information provided above, we can see that the best Accuracy on the training set is of 65.6%. This is achieved by using a complexity parameter of 0.015.

## Confusion Matrix

We are now going to look on the performance on the test set.

```{r}
confusionMatrix(predict.train(ct.caret, newdata = german.te), 
                german.te$RESPONSE)


``` 

As we can see, the overall Accuracy on the test set is of 60.4%. However, one can observe that even though the prediction capacity on each class is more balanced than if we had not balanced the data, the Sensitivity class is better predicted (65.9%) compared to the Specificity class (57.7%). This is confirmed in the Confusion Matrix as well. To further explain, the "1" class is correctly predicted 97 times out of 168 times. Whereas, the "0" class is correctly predicted 54 times out of 82 times. This means that the model performs well in predicting bad credit rating candidates. However, if the aim of the project is to uncover good credit rating candidates based on the explanatory variables, the model will do a poorer job.

## Tree drawing
  
We now plot the final and best model. 
```{r, fig.height = 5, fig.width = 10}
par(mar = c(0,0,0,0))
plot(ct.caret$finalModel)
text(ct.caret$finalModel)
```

# Neural Network

The second model that we considered is the Neural Network. Similarly to the Decision Tree, we scaled and balanced the data, as well as, preformed a repeated cross-validation, in order to achieve better results. To go further, we also tuned the "size" and "decay" hyperparameters.

```{r, echo=FALSE, include = FALSE}
set.seed(2006)

hp_nn <- expand.grid(size = 2:4, decay = seq(0, 0.5, 0.05))

credit.nn.caret <- train (
  form = RESPONSE ~ .,
  data = german.tr,
  method = "nnet",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    verboseIter = FALSE,
    sampling = "down"),
  tuneGrid = hp_nn
)

```


## Best Parameters
  
```{r}
credit.nn.caret$bestTune 
print(credit.nn.caret)
plot(credit.nn.caret)
```
As we can see, the highest Accuracy (69.3%) on the training set can be obtained by setting size = 4 and decay = 0.5.

## Confusion Matrix
```{r}
confusionMatrix(predict.train(credit.nn.caret, newdata = german.te), 
                german.te$RESPONSE)
``` 
The results on the test set are slightly better than for the Decision Tree. Overall we obtain an Accuracy of 65.6%. The prediction of each class is balanced even though the Specificity class (66.7%) is better predicted than the Sensitivity class (63.4%), unlike for the Decision Tree model. 

The results are confirmed in the Confusion Matrix where the "1" class is correctly predicted 112 times out of 168 times. Whereas, the "0" class is correctly predicted 52 times out of 82 times. In this way, the Neural Network outperforms the Decision Tree model in predicting the Specificity class (1). However, the results are almost equivalent for the Sensitivity class (0).

## Neural Network Plotting
  
```{r, fig.width = 15}
library(NeuralNetTools)
par(mar = c(0,0,0,0))
plotnet(credit.nn.caret, pos_col = "darkgreen", neg_col = "darkblue")

```

# Logistic Regression

The third model that we considered is the Logistic Regression, that is a powerful classifier for binary outputs.

```{r, warning = FALSE}
set.seed(1)
credit.log.caret <-
  train(
    form = RESPONSE ~ .,
    data = german.tr,
    method = "glm",
    preProcess = c("center", "scale"),
    trControl = trainControl(
      method = "repeatedcv",
      number = 10,
      repeats = 10,
      verboseIter = FALSE,
      sampling = "down"
    )
  )


```


## Accuracy measure
  
```{r}
print(credit.log.caret)
summary(credit.log.caret)
```
The results of the training show that not all variables significantly impact the RESPONSE variable. For instance, according to the model, variables such as JOB or NEW_CAR do not impact the RESPONSE variable significantly. Other variables may impact the explained variable at different thresholds. For example, CHK_ACCT3 has a significant influence at a threshold of 0.1%, whereas EMPLOYMENT4, INSTALL_RATE4 and HISTORY4 have impacts at thresholds of, respectively, 1%, 5% and 10%.

Overall, the model presents a good Accuracy (70.9%) on the training set. However, to truly evaluate its prediction capacity we would need to consider the Accuracy on the test set.

## Confusion Matrix
```{r}
confusionMatrix(predict.train(credit.log.caret, newdata = german.te),
                german.te$RESPONSE)

``` 
The analysis of the predictions on the test set reveals that it is as well a good model since the overall Accuracy is of 68.8%. This means, that overall the Logistic Regression outperforms both the Decision Tree and the Neural Network.

When looking in more details, we can see that both classes are more or less well predicted. Similarly to the Decision Tree model, the Sensitivity class (73.2%) is better predicted than the Specificity class (66.7%). The results on the Confusion Matrix confirm this. To further explain, the "1" class is correctly predicted 112 times out of 168. Whereas, the "0" class is accurately predicted 60 times out of 82.

This means, that for predicting the Specificity class ("1"), the Logistic Regression performs better than the Decision Tree, but equals the performance of the Neural Network. When looking at the Sensitivity class ("0"), this model outperforms both the Neural Network and the Decision Tree. In other words, when only considering these three models seen up to now, the Neural Network and the Logistic Regression would be the better models to find good credit candidates. On the other hand, if the aim of the project is to detect bad credit candidates, then the Logistic Regression model should be preferred. Overall, this confirms that the Logistic Regression does a better job than both the Neural Network and the Decision Tree.

Let's look further into other models.

# Discriminant Analysis

The fourth model considered is the Discriminant Analysis. First, we are going to use a Linear Discriminant Analysis (LDA), then a Quadratic Discriminant Analysis (QDA).

## LDA

```{r, message= FALSE, warning=FALSE}
set.seed(4567)
lda.fit <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="lda",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling = "down")
                         )
```

After having trained the model, we are going to make predictions on the test set and look at the results.

### Confusion Matrix
```{r}
lda.pred <- predict(lda.fit, GermanCredit)

confusionMatrix(predict.train(lda.fit, newdata = german.te),
                german.te$RESPONSE)
```

One can observe that the Accuracy is of 68.4%. So far, this is a good model. 

Looking into more details, we note that the Sensitivity and the Specificity are respectively of 73.2% and 66.1%. The model was able to predict 60 times out of 82 a bad credit, and 111 times out of 168 a good credit. Comparing with the other models, the LDA has similar prediction capacity as the Logistic Regression for the class ("1"), meaning the good credits, as well as, for the prediction of class ("0").

## QDA

We are now going to create a model using a Quadritic Discriminant Analysis.

```{r warning= FALSE}

set.seed(1)
qda.fit <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="qda",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling = "down")
                         ) 


qda.fit
```

When training the model, the Accuracy is at 68% which is lower compared to the Linear Discriminant Analysis Model. We are still going to see how the model performs when fitted on the test dataset.

### Confusion Matrix
```{r}
qda.pred <- predict(qda.fit, GermanCredit)

confusionMatrix(predict.train(qda.fit, newdata = german.te),
                german.te$RESPONSE)

```

When the model is fitted on the test set, the Accuracy is of 71.2%, which is the highest level achieved so far. However when looking at the Sensitivity, one can remark that the it has a low value (60.98%). The model is able has predicted 50 bad credits out of 82, making him the worst in terms of prediction capability of the class "0". On the other hand, the Specificity is of 76.19%, the highest value achieved for the "1" class. 

Hence, we can say that the model is very good at predicting the good credits ("1"), but very bad a predicting the bad ones. Even if the Accuracy is high, it is preferable to use a model that is able to better predict both classes. 

# Support Vector Machine I

The next model is the Support Vector Machine. We will first use the Radial, then the Poly model.

```{r, warning = FALSE, message = FALSE}
library(doParallel) # use the parallel backend
cl<-makeCluster(detectCores()) # detect and create a cluster
registerDoParallel(cl)

C <- c(0.25, 0.1, 0.5, 1, 10, 100)
sigma <- c(0.0001, 0.001, 0.01, 0.1, 1)
gr.radial<-expand.grid(C = C, sigma = sigma)

set.seed(123)
system.time(model_svm<-caret::train(RESPONSE ~ .,
                          data = german.tr,
                          method = "svmRadial",
                          preProcess = "range",
                          trace=FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE, sampling = "down"),
                          tuneGrid=gr.radial))
model_svm
model_svm$bestTune
plot(model_svm)

```

As we can see from the model summary and later confirmed using the best Tune and the plot, the model is well fitted to the data when Sigma = 0.0001 and the Cost C = 0.25. As we can see, the Cost is very small. Having a small Cost means that the margins are going to be very large. The risk of having such a small Cost is that the model is under-fitted. If that is the case, we would not be able to well predict the outcome variable on the test set. We can look at the Confusion Matrix and see what happens.

## Confusion Matrix
```{r, warning = FALSE, message = FALSE}

confusionMatrix(predict.train(model_svm, newdata = german.te),
                german.te$RESPONSE)
```

The model's Accuracy is of 70%. However, when looking at the Sensitivity and the Specificity, we realise that it is unable to balance the classes despite having set "sampling = down". To further explain, the Sensitivity is very low (28,05%). Indeed the model, correctly predicts the "0" class 23 times out of 82. On the other hand, this model is very good at predicting the "1" class. In fact, it achieves a Specificity score of 90.48%.

This means that the model is very good at predicting good creditors but does a very poor jobs at predicting bad ones. In the light of what we have seen, we can confirm that the model is under-fitting. Indeed, it is unable to predict correctly the "0" class.

# Support Vector Machine II

After having fitted a first Support Vector Machine model, we are now going to fit another one using "svmPoly" and see if the results improve. 

```{r, warning=FALSE, message=FALSE}
C <- c(0.1, 1, 10, 100)
degree <- c(1, 2, 3)
scale <- 1

gr.poly <- expand.grid(C = C, degree = degree, scale = scale)
ctrl <- trainControl(method = "cv",
                     number = 3,
                     sampling = "down")

set.seed(123)
system.time(
  model_svm_poly <-
    train(
      RESPONSE ~ .,
      data = german.tr,
      method = "svmPoly",
      trControl = ctrl,
      tuneGrid = gr.poly
    )
)

model_svm_poly

model_svm_poly$bestTune
plot(model_svm_poly)
```

This Support Vector Machine model does not only tune the Cost parameter, but also the Scale and the Degree. The output of the model's summary, as well as the bestTune function and the graph show that the model gives the best results when Degree = 1, Scale = 1, and C = 0.1. We are now going to analyse the Confusion Matrix and see if the model's prediction capacity are good.


## Confusion Matrix
```{r, warning = FALSE, message=FALSE}

confusionMatrix(predict.train(model_svm_poly, newdata = german.te),
                german.te$RESPONSE)
```

This Support Vector Machine model has an Accuracy of 66.8%. It is a bit lower than the Radial one. However, we can highlight that, even though it does a poorer job, it predicts both classes in a more balanced way. This one is slightly better at predicting the bad creditors ("0") compared to the good ones. Indeed, the model is able to predict correctly bad creditors 56 times out of 82 and good creditors 111 times out of 168.


# Ensemble Methods (Random Forest)

The last model we are going to analyse is the Random Forest. 

```{r, warning=FALSE, message=FALSE}
set.seed(1994)
modelLookup(model="rf")

model_rf <- caret::train(RESPONSE ~ .,
                         data=german.tr,
                         method="rf",
                         preProcess=c("center", "scale"),
                         trControl=trainControl(method="repeatedcv", number=10,
                         repeats=10, verboseIter=FALSE, sampling="down")
                         )
model_rf

```

The output shows that the best Accuracy can be reached when mtry = 18. Indeed, the model has been doing different simulation with different mtry (Number of variables randomly sampled as candidates at each split), and ended by fitting the model to the training set using mtry = 18. Indeed, when we run the best tune we obtain the following:


```{r}
model_rf$bestTune
```

We are now going to look at how the model behaves when predicting the output from the test set.

## Confusion Matrix
```{r}
confusionMatrix(predict.train(model_rf, newdata = german.te),
                german.te$RESPONSE)
```
The model's Accuracy is of 69.6%.

When looking at the Sensitivity and the Specificity, one can note that the model is better at predicting the bad credits ("0"). Indeed, the Sensitivity is of 75.6% while the Specificity of 66.7%. So when we fitted as random forest, we remark that 112 times out of 168 the model is able to predict good credits and 62 times out of 82, a bad one. Overall, the model is satisfying as it is able to predicted both classes pretty well. 

Generally, to have even better results we could make a variable importance analysis. However, having used the Bortuta Algorithm at the beginning of our analysis, this step is no longer necessary. 

# Conclusion

To put in a nutshell, we have observed that overall most of the models are predicting quite well both classes. When considering the top 3 in terms of overall Accuracy, the ones that stand out are the QDA model (71.2%),  followed by the Radial SVM model (70.0%), and Random Forest model (69.6%). 

However, depending on the aim of the project, the choice of model may differ. To further explain, if the goal is to predict good credit candidates, one should consider looking at the models having the highest Specificity rates, that are, in this case, the same as for the overall Accuracy: the Radial SVM (90.5%), followed by the QDA model (76.19%) and the Random Forest model (66.7%). 

On the other hand, if the aim of the project is to detect and predict bad creditors, then the models considered should be the ones having the highest Sensitivity rates:the Random Forest model (75.61%), followed by the Logistic Regression and LDA models (both at 73.2%).

However, it is better to identify the bad creditors in order to avoid lending money to people who are not going to pay back. Hence, the model you should use to define if a client can be considered as a good or a bad creditor is the one with the highest sensitivity, thus, the random forest.